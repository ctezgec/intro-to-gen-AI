{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce088aed",
   "metadata": {},
   "source": [
    "# Chapter 1: build the backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3f6678",
   "metadata": {},
   "source": [
    "In this chapter you are going to build a simple program that can be executed from the terminal. The program connects the user to an LLM via the OpenAI API (Application Programming Interface)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94940cc",
   "metadata": {},
   "source": [
    "## Create your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2d880",
   "metadata": {},
   "source": [
    "Create an environment, then use it in the jupyter notebook. You don't need to install any packages for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74bd12a",
   "metadata": {},
   "source": [
    "## Create an OpenAI account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96727b",
   "metadata": {},
   "source": [
    "Go to openai.com, create a developer account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab9326",
   "metadata": {},
   "source": [
    "## Save your credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a471c3",
   "metadata": {},
   "source": [
    "After creating your account, you will have to activate your OpenAI key. Create a file called \".env\" and copy and paste your key there, like: API_KEY=pcq9e92n0icq.\n",
    "\n",
    "Check that your API key is stored in the file by running the following snippet. You will reuse this function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d68b051f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: sk-pr\n"
     ]
    }
   ],
   "source": [
    "# How to fetch your API key from the .env file\n",
    "def get_env_value(key, filepath='.env'):\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip() and not line.startswith('#'):  # Skip comments/empty lines\n",
    "                k, v = line.strip().split('=', 1)  # Split on first '=' only\n",
    "                if k == key:\n",
    "                    return v\n",
    "    raise ValueError(f\"Key '{key}' not found in {filepath}\")\n",
    "\n",
    "api_key = get_env_value('API_KEY')\n",
    "print(\"API Key:\", api_key[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53aefa",
   "metadata": {},
   "source": [
    "## Explore the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46beaa50",
   "metadata": {},
   "source": [
    "Go to the OpenAI website and explore the documentation for the Python API. Play with the examples for a bit to get the feeling of how it works. **Try to send a message to one of the LLMs (choose the cheaper models) and print the reply.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed6b4945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a panda sitting on the grass, surrounded by rocks and green bushes. The panda is holding and eating bamboo.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    input = [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\": [\n",
    "            {\"type\":\"input_text\", \"text\":\"what is in this image?\"},\n",
    "                    {\"type\":\"input_image\",\"image_url\":\"https://upload.wikimedia.org/wikipedia/commons/6/68/Ailuropoda_melanoleuca_%28Panda_g%C3%A9ant%29_-_445.jpg\"},\n",
    "                    ],\n",
    "        }],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5a081",
   "metadata": {},
   "source": [
    "## Create your program!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089b960",
   "metadata": {},
   "source": [
    "### LLM call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e8d11",
   "metadata": {},
   "source": [
    "Your task is to create a function that calls the OpenAI API, passing the user input and returning the LLM output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6acdf02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07e832be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(user_input:str)->str:\n",
    "\n",
    "    llm_output = client.responses.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        input = user_input\n",
    "    \n",
    "    )\n",
    "\n",
    "    #print(llm_output.output_text)\n",
    "\n",
    "    return llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaf44be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely! Pineapples on pizza is a popular topping known for adding a sweet and tangy flavor. It's often used in the classic Hawaiian pizza, which combines pineapple with ham. While some people love it, others prefer more traditional toppings. Ultimately, whether you put pineapples on your pizza depends on your personal taste. Do you enjoy pineapple on pizza?\n"
     ]
    }
   ],
   "source": [
    "generate(\"can we put pineapples on pizza\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de4da3",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994de7fb",
   "metadata": {},
   "source": [
    "Now create a main function. In the main function, the program asks an input from the user, then invokes the previous function to generate a reply from the LLM, and prints it. You can use a loop to keep the chat open until the user is dissatisfied and inputs \"/bye\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8792ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main()->None:\n",
    "    api_key = get_env_value('API_KEY')\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    a = input(\"Enter your input: \")\n",
    "    while True:\n",
    "        if a == \"/bye\":\n",
    "            break\n",
    "        else:\n",
    "            llm_output = generate(a)\n",
    "            print(llm_output.output_text)\n",
    "            a = input(\"Enter your input: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c696a06",
   "metadata": {},
   "source": [
    "Try to put everything in a .py file, so that it can be called in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e67e866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name **Emanuele** is of Italian origin and is a variation of the name **Emmanuel**, which means \"God is with us.\" It is derived from the Hebrew name **Immanuel** (עִמָּנוּאֵל, *Immanu'el*), composed of the elements **'immanu'** (\"with us\") and **'El'** (\"God\"). The name is often used in Christian contexts, referencing the biblical prophecy of Emmanuel.\n",
      "\n",
      "**In Summary:**\n",
      "- **Emanuele** means **\"God is with us\"**.\n",
      "- It is the Italian form of **Emmanuel**.\n",
      "\n",
      "Would you like more information about the name's history or usage?\n"
     ]
    }
   ],
   "source": [
    "# Add the following line to execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a910b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate venv: .\\.venv\\Scripts\\Activate.ps1\n",
    "# update the venv: pip install -r requirements.txt --upgrade --no-cache-dir\n",
    "# update requirements.txt: pip freeze --exclude-editable > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
